#mongo精简版本说明


脚本启动方式：
脚本 main_fun.py是总程序入口


脚本说明：
#在配置文件中选择zmq,http通信
由于断连问题提供了下面的检测机制
    main_fun.py中会负责启动所有功能脚本，并且负责检查与服务器的连接状态检测，如果到了检测时间发现就绪队列与服务器交互的本地任务达到某个数值
（这个数值理想状态是小于等于1的，生产后会立即被消费），就认为与服务器断连，会杀掉启动的功能进程，重启启动功能进程，
并且更新本地的状态和执行时间（更新的时间为当前时间，之所以更新时间是防止与服务器断连时间过久，会连续扫描频繁访问服务器）



client整体重要目录结构说明：

                                        |
                                        |--------get_crawltask_inter.py#用户选择go实现的抓取器时该脚本生效，对抓取器的接口
                                        |
                                        |--------delay_queue.py （负责扫描就绪任务，填充到就绪队列）
            |--------client_doc-------- |
            |                           |--------scan_process.py (开启多进程，获取任务，并且调用相应的任务脚本)
            |                           |--------setting.py(总配置文件)
            |
            |                           |
            |---------excutor---------- |--------excutor_main.py （python抓取器脚本）
            |(抓取器目录，存放抓取器脚本)   |
            |                           |
            |                           |--------gripper_for_http (go抓取器脚本，可执行的二进制文件)
            |                           |
            |                           |--------gripper_for_http_conf.json(与go抓取器匹配的配置文件必须与go抓取器在同一目录)
client----- |
 |          |                           |--------excutor_main.py(该脚本封装了生成抓取任务，和获得解析任务的接口(该接口使用了两种方式实现，yield_interface,interface,为接口名称，该脚本有详细说明)。和存储解析结果的接口（大于16M,小于16M,html）)
            |                           |
            |                           |---------db_oprate.py(该脚本了封装了，同步操作pymongo的所有数据库操作。而excutor/excutor_main.py与client_doc/delay_queue.py都为异步操作没有封装进来)
            |                           |
            |---------excutor_doc-------|---------system_info.py（获取客户端硬件信息的本地任务脚本）
            |(存放任务脚本依赖模块的目录)   |
            |                           |---------other (*.py都为script_main目录下功能脚本依赖的模块)
            |
            |                           |---------http_local_task.py（使用http与服务器交互的脚本，任务类型默认对应'http_local_task'）
            |--------script_main--------|
            |                           |
            |                           |---------zmq_local_task.py（使用zmq与服务器交互的脚本，任务类型默认为'zmq_local_task'）
            |                           |
            |                           |
            |                           |
            |                           |---------other(*.py *对应脚本名，与类型。其他脚本与服务器下发相关)
            |
            |
            |---------create_task.py（测试脚本，模拟服务器下发的任务，生产环境剔除）
            |
            |
            |----------main_fun.py(总程序入口)
            |



注：1.以上目录所列文件除了other，都为平台提供。other为任务脚本
    2.关于setting.py说明，目前服务器和客户端都支持了（zmq,http）通信方式，选择通信方式时服务器端与客户端的通信方式必须保持一致,推荐使用http

关于抓取器说明：
目前客户端支持使用本地python抓取器，与go抓取器，go抓取器不管是本地和远端都使用的是http通信，所以在使用go抓取器时要关心ip和port。


关于运行说明：

追求最大的运行速率：
       excutor目录下的抓取器脚本可以开启多个进程，目前默认开启一个，它与client_doc 目录下的scan_process.py密切相关，scan_process.py
会获取服务器任务生成抓取任务，而抓取任务会被抓取器脚本获得，然后分解，进入内存（请关注这里的内存，下方会进行说明）。也就是说scan_process.py
脚本下开启获取任务的进程数与线程数之和获取的任务都会进入内存。而由于抓取器的并发数量有限，并且考虑极端防爬条件，抓取的速率也就是释放内存的速率
与进入内存的任务前者远远小于后者，也就是形成了内存堆叠。到这里你肯定在想为什么不开启多个抓取器这样就可以保证，在内存中的任务数量维持在一个稳定
的数值，这个数值是scan_process.py开启的并发数*任务大小。scan_process.py开启的并发数 = 开启的抓取器脚本数量可以得到最大的运行速率。可以
达到即生产即消费，内存占用稳定。这只是理想状态，如果设备的硬件条件很好，可以考虑上述方案。。

    scan_process.py开启的并发数 = 开启的抓取器脚本数量可以得到最大的运行速率。但是我们并不能采用这种方案，由于进程的开销之大，而我们的设备
可能没被任务把内存占满，进程的开销反而影响了整体设备的内存和cpu。所以合理的设置scan_process.py脚本开启的并发数，与开启抓取器脚本的数量将能
最大的利用已有的设备。并且能长期运行。


以默认设置用公式简单说明该功能代码常驻内存的大小。
scan_process.py 并发数为  8
抓取器          开启       1

常驻内存为： 8*抓取任务*分解的url*html文件   （如一个抓取任务可以分解800个url,url全部抓取成功，每个htmlw文件假设200k,一个任务占用内存156M）
           按上诉计算8个任务最大内存大约占1.2G内存。为了减少内存生成的抓取任务，应该尽量少的生成url.



关于上传数据的说明：

目前实现方式：（以http为例说明）
    客户端以16M（由于mongo文档的限制）为界限，提供了大于和小于16M的存储方式，按类型的存储方式。http协议上传数据体长度是无限制的，但是由于各个web框架对数据体的长度作出了限制，所以
不得不对上传数据的长度进行一定的限制。

而针对上诉限制，一次上传的数据不能太小（太小影响整体上传速度），也不能太大（太大webserver可能拒绝接收）。

解决方案：

服务器解决：
    服务器将评估每个任务最终生成的数据体长度，将大任务拆分成适中的小任务。将同类型小任务拼接成一个大任务。确保任务的数据量不会太大。
这样即减少了任务总数（单位时间上传的数据有限，减少任务量相当于提高了上传速度），又限制了上传数据的大小。

客户端解决：
    拆分任务产生的数据，分成大小适中的若干小块，然后上传小块数据。由于单位时间上传的数据块有限，过度细化将极大的拉低上传速度。服务器再进行数据的拼接。


整体项目遇到的问题：

关于内存占用率的问题：
1：通过测试确定python抓取器中的aiohttp的内存占用率很高。
mac测试抓取器的结果：在mac通过持续运行测试pthon抓取器的内存一直趋于增长，运行到一段时间会释放，但是释放的内存并不多，内存不能得到及时释放。
linux测试抓取器结果：在linux上测试python抓取器的内存占用稳定在某一数值就不会变化，但是该数值较大，加上其他代码的内存占用，整体项目占用的内存在1.5G--2G左右。
2：mongo同样是占用内存使用的大户。
关于磁盘空间占满的问题：
将mongodb的mongodb.conf配置文件的日志功能关闭，如果不关闭的话它会持续将日志写入文件占满磁盘，测试发现7天左右日志文件已经达到400G。
